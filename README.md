# ğŸ§ª Fine-Grained Evaluation of Visual Language Models (VLMs) using LLM-as-Judge

This project investigates the effectiveness of **small, non-fine-tuned language models** (â‰¤15B parameters) for **fine-grained evaluation** of Visual Language Models (VLMs). The study explores prompt engineering strategies and scoring schemes to assess the accuracy and alignment of VLM responses with human judgment.

---

## ğŸ“Œ Abstract

Visual Language Models (VLMs) generate text from images, often with varied structure and abstraction. Evaluating their responses remains a challenge due to:
- Coherent but unstructured answers,
- Diverse response styles,
- Lack of large-scale, human-evaluation-friendly methods.

This study explores the use of **LLM-as-a-Judge** using **small open-source LLMs** for fine-grained VLM evaluation. Experiments show **up to 94.3% accuracy** and **0.91 correlation** with human judgments when using structured prompts and a robust evaluation rubric.

---

## ğŸ§  Research Questions
1. How effective is LLM-as-a-Judge for VLM evaluation?
2. What are the limitations and biases?
3. How can we improve evaluation fidelity?
4. Are complex, fine-grained rubrics better than simple string matching?

---

## ğŸ§° Dataset
- **25 VLM responses** generated by **LLaVA v1.5 Vicuna-7B**.
- Each VLM response describes an **animal image**, with the task: _â€œIdentify the animal as specifically as possibleâ€_.
- Ground truths are annotated with **Class**, **Family**, and **Species**.

---

## ğŸ¤– LLMs Used as Judges
- âœ… Non-fine-tuned LLMs only (â‰¤15B)
- âœ… Models from HuggingFace
- âœ… Example models:
  - `Prometheus-13B-v1.0`
  - `Mistral-7B-Instruct`
  - `Phi-3-Medium-128k`
  - `Mixtral-8x7B-Instruct`
  - `Hermes-2-Theta-LLaMA3-8B`

---

## ğŸ” Project Structure

```
â”œâ”€â”€ data/ # Evaluation data
â”‚ â”œâ”€â”€ dataset.jsonl/ # Dataset for evaluation
â”œâ”€â”€ models/ # Config files and model initialisation scripts
â”œâ”€â”€ evaluation/
â”‚ â”œâ”€â”€ Eval_1.ipynb/ # Evaluation script for 1st evaluation strategy
â”‚ â””â”€â”€ Eval_2.ipynb/ # Evaluation script for 2nd evaluation strategy
â”‚ â””â”€â”€ Eval_3.ipynb/ # Evaluation script for 3rd evaluation strategy
â”œâ”€â”€ result/ # Results from the evaluation
â”‚ â”œâ”€â”€ eval_1.ipynb/ # Evaluation results for 1st evaluation strategy
â”‚ â””â”€â”€ eval_2.ipynb/ # Evaluation results for 2nd evaluation strategy
â”‚ â””â”€â”€ eval_3.ipynb/ # Evaluation results for 3rd evaluation strategy
â”œâ”€â”€ report/ # Final report and appendix
â””â”€â”€ README.md
```

---

## ğŸ”¬ Evaluation Strategy

### Evaluation 1:  
- **Simple scoring** (0 to 3) based on match with Class, Family, Species.
- Issues: rigid criteria, hallucinations, low scalability.

### Evaluation 2:  
- **Stepwise evaluation** (Chain-of-Thought):
  1. Keyword extraction
  2. Classification
  3. Scoring
- Issues: better structure but inconsistent across steps.

### Evaluation 3:  
- **Unified evaluation prompt**
- Improved scoring rubric across 5 metrics:
  - Class correctness
  - Family correctness
  - Species correctness
  - Descriptive accuracy
  - Relevance

---

## ğŸ“Š Results Summary

### Accuracy (Evaluation 3):
| Model          | Accuracy Range    |
|----------------|-------------------|
| Phi-3          | **94.3% â€“ 100%**   |
| Mixtral        | 87.5% â€“ 96.7%      |
| Hermes         | 85.5% â€“ 95.5%      |
| LLaMA-2        | **61.6% â€“ 92.5%**  |

### Correlation with Human Scores:
- Highest: **0.91 (Phi)**
- Lowest: 0.11 (LLaMA-2)

---

## ğŸ” Key Insights
- Fine-grained prompts and evaluation rubrics outperform simple string match.
- Few-shot examples improve LLM judge consistency.
- Small open-source models are capable LLM judges if well-instructed.
- Prompt structure, task clarity, and evaluation rubric design are **critical**.

---

## âš ï¸ Limitations
- Dataset focused only on **bird species**, size = 25 samples.
- Only one human annotator used as ground truth.
- No multi-modal input evaluation (e.g. images not fed to judge).

---

## ğŸ”­ Future Work
- Extend dataset to other categories and larger sizes.
- Compare small vs. large LLM judges on diverse tasks.
- Integrate image-to-text preprocessing pipelines.
- Study robustness to bias in judges (e.g. verbosity bias, position bias).

---

## ğŸ“ Citation

```
@misc{chouguley2024vlmeval,
author = {Sarang Ravi Chouguley},
title = {Fine-Grained Evaluation of VLMs using LLM-as-Judge},
year = {2024},
url = {https://github.com/SarangChougs/fine-grained-evaluation-vlms}
}
```
---

## ğŸ“¬ Contact

For questions or collaboration, feel free to reach out at:  
ğŸ“§ sarangchouguley284@example.com