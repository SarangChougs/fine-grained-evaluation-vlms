# 🧪 Fine-Grained Evaluation of Visual Language Models (VLMs) using LLM-as-Judge

This project investigates the effectiveness of **small, non-fine-tuned language models** (≤15B parameters) for **fine-grained evaluation** of Visual Language Models (VLMs). The study explores prompt engineering strategies and scoring schemes to assess the accuracy and alignment of VLM responses with human judgment.

---

## 📌 Abstract

Visual Language Models (VLMs) generate text from images, often with varied structure and abstraction. Evaluating their responses remains a challenge due to:
- Coherent but unstructured answers,
- Diverse response styles,
- Lack of large-scale, human-evaluation-friendly methods.

This study explores the use of **LLM-as-a-Judge** using **small open-source LLMs** for fine-grained VLM evaluation. Experiments show **up to 94.3% accuracy** and **0.91 correlation** with human judgments when using structured prompts and a robust evaluation rubric.

---

## 🧠 Research Questions
1. How effective is LLM-as-a-Judge for VLM evaluation?
2. What are the limitations and biases?
3. How can we improve evaluation fidelity?
4. Are complex, fine-grained rubrics better than simple string matching?

---

## 🧰 Dataset
- **25 VLM responses** generated by **LLaVA v1.5 Vicuna-7B**.
- Each VLM response describes an **animal image**, with the task: _“Identify the animal as specifically as possible”_.
- Ground truths are annotated with **Class**, **Family**, and **Species**.

---

## 🤖 LLMs Used as Judges
- ✅ Non-fine-tuned LLMs only (≤15B)
- ✅ Models from HuggingFace
- ✅ Example models:
  - `Prometheus-13B-v1.0`
  - `Mistral-7B-Instruct`
  - `Phi-3-Medium-128k`
  - `Mixtral-8x7B-Instruct`
  - `Hermes-2-Theta-LLaMA3-8B`

---

## 🔍 Project Structure

```
├── data/ # Evaluation data
│ ├── dataset.jsonl/ # Dataset for evaluation
├── models/ # Config files and model initialisation scripts
├── evaluation/
│ ├── Eval_1.ipynb/ # Evaluation script for 1st evaluation strategy
│ └── Eval_2.ipynb/ # Evaluation script for 2nd evaluation strategy
│ └── Eval_3.ipynb/ # Evaluation script for 3rd evaluation strategy
├── result/ # Results from the evaluation
│ ├── eval_1.ipynb/ # Evaluation results for 1st evaluation strategy
│ └── eval_2.ipynb/ # Evaluation results for 2nd evaluation strategy
│ └── eval_3.ipynb/ # Evaluation results for 3rd evaluation strategy
├── report/ # Final report and appendix
└── README.md
```

---

## 🔬 Evaluation Strategy

### Evaluation 1:  
- **Simple scoring** (0 to 3) based on match with Class, Family, Species.
- Issues: rigid criteria, hallucinations, low scalability.

### Evaluation 2:  
- **Stepwise evaluation** (Chain-of-Thought):
  1. Keyword extraction
  2. Classification
  3. Scoring
- Issues: better structure but inconsistent across steps.

### Evaluation 3:  
- **Unified evaluation prompt**
- Improved scoring rubric across 5 metrics:
  - Class correctness
  - Family correctness
  - Species correctness
  - Descriptive accuracy
  - Relevance

---

## 📊 Results Summary

### Accuracy (Evaluation 3):
| Model          | Accuracy Range    |
|----------------|-------------------|
| Phi-3          | **94.3% – 100%**   |
| Mixtral        | 87.5% – 96.7%      |
| Hermes         | 85.5% – 95.5%      |
| LLaMA-2        | **61.6% – 92.5%**  |

### Correlation with Human Scores:
- Highest: **0.91 (Phi)**
- Lowest: 0.11 (LLaMA-2)

---

## 🔍 Key Insights
- Fine-grained prompts and evaluation rubrics outperform simple string match.
- Few-shot examples improve LLM judge consistency.
- Small open-source models are capable LLM judges if well-instructed.
- Prompt structure, task clarity, and evaluation rubric design are **critical**.

---

## ⚠️ Limitations
- Dataset focused only on **bird species**, size = 25 samples.
- Only one human annotator used as ground truth.
- No multi-modal input evaluation (e.g. images not fed to judge).

---

## 🔭 Future Work
- Extend dataset to other categories and larger sizes.
- Compare small vs. large LLM judges on diverse tasks.
- Integrate image-to-text preprocessing pipelines.
- Study robustness to bias in judges (e.g. verbosity bias, position bias).

---

## 📝 Citation

```
@misc{chouguley2024vlmeval,
author = {Sarang Ravi Chouguley},
title = {Fine-Grained Evaluation of VLMs using LLM-as-Judge},
year = {2024},
url = {https://github.com/SarangChougs/fine-grained-evaluation-vlms}
}
```
---

## 📬 Contact

For questions or collaboration, feel free to reach out at:  
📧 sarangchouguley284@example.com